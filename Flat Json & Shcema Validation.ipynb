{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Untitled0.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 685
        },
        "id": "JaaCTMyonUap",
        "outputId": "a5bf6791-55b5-49fb-a6e1-85cab7138e6f"
      },
      "source": [
        "!apt-get update\r\n",
        "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\r\n",
        "!wget -q htt  https://archive.apache.org/dist/spark/spark-2.3.1/spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!tar xf spark-2.3.1-bin-hadoop2.7.tgz\r\n",
        "!pip install -q findspark\r\n",
        "\r\n",
        "import os\r\n",
        "os.environ[\"JAVA_HOME\"] = \"/usr/lib/jvm/java-8-openjdk-amd64\"\r\n",
        "os.environ[\"SPARK_HOME\"] = \"/content/spark-2.3.1-bin-hadoop2.7\"\r\n",
        "\r\n",
        "import findspark\r\n",
        "findspark.init()\r\n",
        "from pyspark.sql import SparkSession\r\n",
        "spark = SparkSession.builder.getOrCreate()\r\n",
        "spark"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\r0% [Working]\r            \rIgn:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r                                                                               \rGet:2 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ InRelease [3,626 B]\n",
            "\r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [2 \r0% [Connecting to archive.ubuntu.com (91.189.88.142)] [Waiting for headers] [Wa\r0% [2 InRelease gpgv 3,626 B] [Connecting to archive.ubuntu.com (91.189.88.142)\r                                                                               \rGet:3 http://security.ubuntu.com/ubuntu bionic-security InRelease [88.7 kB]\n",
            "\r                                                                               \rGet:4 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic InRelease [15.9 kB]\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rIgn:5 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  InRelease\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rHit:6 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu1804/x86_64  Release\n",
            "\r0% [2 InRelease gpgv 3,626 B] [Waiting for headers] [3 InRelease 14.2 kB/88.7 k\r                                                                               \rGet:7 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release [564 B]\n",
            "Get:8 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Release.gpg [833 B]\n",
            "Hit:9 http://archive.ubuntu.com/ubuntu bionic InRelease\n",
            "Get:10 http://archive.ubuntu.com/ubuntu bionic-updates InRelease [88.7 kB]\n",
            "Hit:11 http://ppa.launchpad.net/cran/libgit2/ubuntu bionic InRelease\n",
            "Get:12 https://cloud.r-project.org/bin/linux/ubuntu bionic-cran40/ Packages [43.2 kB]\n",
            "Get:13 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic InRelease [21.3 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu bionic-backports InRelease [74.6 kB]\n",
            "Get:16 https://developer.download.nvidia.com/compute/machine-learning/repos/ubuntu1804/x86_64  Packages [66.5 kB]\n",
            "Get:17 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main Sources [1,707 kB]\n",
            "Get:18 http://security.ubuntu.com/ubuntu bionic-security/main amd64 Packages [1,869 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu bionic-updates/restricted amd64 Packages [304 kB]\n",
            "Get:20 http://security.ubuntu.com/ubuntu bionic-security/restricted amd64 Packages [261 kB]\n",
            "Get:21 http://security.ubuntu.com/ubuntu bionic-security/universe amd64 Packages [1,376 kB]\n",
            "Get:22 http://ppa.launchpad.net/c2d4u.team/c2d4u4.0+/ubuntu bionic/main amd64 Packages [874 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 Packages [2,140 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu bionic-updates/main amd64 Packages [2,298 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu bionic-updates/multiverse amd64 Packages [45.6 kB]\n",
            "Get:26 http://ppa.launchpad.net/graphics-drivers/ppa/ubuntu bionic/main amd64 Packages [49.2 kB]\n",
            "Fetched 11.3 MB in 3s (3,624 kB/s)\n",
            "Reading package lists... Done\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "\n",
              "            <div>\n",
              "                <p><b>SparkSession - in-memory</b></p>\n",
              "                \n",
              "        <div>\n",
              "            <p><b>SparkContext</b></p>\n",
              "\n",
              "            <p><a href=\"http://90ecb8437e1f:4040\">Spark UI</a></p>\n",
              "\n",
              "            <dl>\n",
              "              <dt>Version</dt>\n",
              "                <dd><code>v2.3.1</code></dd>\n",
              "              <dt>Master</dt>\n",
              "                <dd><code>local[*]</code></dd>\n",
              "              <dt>AppName</dt>\n",
              "                <dd><code>pyspark-shell</code></dd>\n",
              "            </dl>\n",
              "        </div>\n",
              "        \n",
              "            </div>\n",
              "        "
            ],
            "text/plain": [
              "<pyspark.sql.session.SparkSession at 0x7fc37b6d3940>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgUfJp9fnVJB"
      },
      "source": [
        "from pyspark.sql.functions import *\r\n",
        "from pyspark.sql.types import *"
      ],
      "execution_count": 87,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2BxblAHonetK",
        "outputId": "ed5411e1-07ff-43f7-a4b5-f0d52151b2e0"
      },
      "source": [
        "df1=spark.read.option(\"multiline\",\"true\").json('/content/1.json')\r\n",
        "df1.show()"
      ],
      "execution_count": 100,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "|  A|\n",
            "+---+\n",
            "|[1]|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LdEMIZdNnmw8",
        "outputId": "c4314a94-6f23-49f8-da8a-c4d1634dfa86"
      },
      "source": [
        "df2=spark.read.option(\"multiline\",\"true\").json('/content/2.json')\r\n",
        "df2.show()"
      ],
      "execution_count": 101,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "|  A|  C|\n",
            "+---+---+\n",
            "|[1]|  2|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gdQv31sLnqoY",
        "outputId": "b4cbd575-e49e-40a7-f1ed-6df2b560fcde"
      },
      "source": [
        "df3=spark.read.option(\"multiline\",\"true\").json('/content/3.json')\r\n",
        "df3.show()"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+------+---+\n",
            "|     A|  C|\n",
            "+------+---+\n",
            "|[1, 3]|  2|\n",
            "+------+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1UgKmVHtnvIq"
      },
      "source": [
        "def all_columns_apart_from_the_particular_column(df,name_string):\r\n",
        "  ms=df.schema.fields;\r\n",
        "  ag=[];\r\n",
        "  for i in ms:\r\n",
        "    if(i.name!=name_string):\r\n",
        "      ag.append(i.name);\r\n",
        "  return ag;"
      ],
      "execution_count": 104,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ApqxcIrmnvyC"
      },
      "source": [
        "def expand_structure(dfstruct,al):\r\n",
        "  ag=[];\r\n",
        "  for i in dfstruct:\r\n",
        "    ag.append(al+\".\"+i.name);\r\n",
        "  return ag[::-1];"
      ],
      "execution_count": 105,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0oSl0vCHnyUO"
      },
      "source": [
        "def flat_df(df):\r\n",
        "  all_raw_fields=df.schema.fields;\r\n",
        "  for i in all_raw_fields:\r\n",
        "    if (str(i.dataType)[0:9])==\"ArrayType\":\r\n",
        "      column_list=all_columns_apart_from_the_particular_column(df,i.name);#Select all the columns apart from the array column\r\n",
        "      explosion=\"explode(\"+i.name+\")\"+ \"as \"+i.name;#Explode the array column\r\n",
        "      column_list.append(explosion);\r\n",
        "      df=df.selectExpr(column_list);\r\n",
        "    elif (str(i.dataType)[0:10])==\"StructType\":\r\n",
        "      column_list=all_columns_apart_from_the_particular_column(df,i.name);#Select all the columns apart from the Structure column\r\n",
        "      dfstruct=df.schema[i.name].dataType.fields;#Select all the fields of Structure Column\r\n",
        "      all_fields_within_structure=expand_structure(dfstruct,i.name);\r\n",
        "      proper_name = [ x+\" as \"+ x.replace('.', \"_\") for x in all_fields_within_structure ];\r\n",
        "      column_list.extend(proper_name)\r\n",
        "      df=df.selectExpr(column_list);\r\n",
        "  return df;"
      ],
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CINg9oasn0bH"
      },
      "source": [
        "def encapsulation(df):\r\n",
        "  ms=str(df.schema);\r\n",
        "  if ((\"ArrayType\" in ms)):#If Arraytype column present , call the flat_df function\r\n",
        "    processing=flat_df(df);\r\n",
        "    return encapsulation(processing);#Recursive Call\r\n",
        "  elif (ms.count(\"StructType\")>1):#If StrctureType column present , call the flat_df function\r\n",
        "    processing=flat_df(df);\r\n",
        "    return encapsulation(processing);\r\n",
        "  else:#Base Case\r\n",
        "    return (df);"
      ],
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PCIgBma5n9B4"
      },
      "source": [
        "dff1=encapsulation(df1);\r\n",
        "dff2=encapsulation(df2);\r\n",
        "dff3=encapsulation(df3);"
      ],
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qAg3RFVPoEqm",
        "outputId": "25110bd4-a729-4491-a17c-016a0d558445"
      },
      "source": [
        "dff1.show()"
      ],
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+\n",
            "|A_B|\n",
            "+---+\n",
            "|  1|\n",
            "+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jdJa4JsaoNjs",
        "outputId": "a4d57fdb-f98f-4b32-cc57-2b69660de07e"
      },
      "source": [
        "dff2.show()"
      ],
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "|  C|A_B|\n",
            "+---+---+\n",
            "|  2|  1|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nh_jB8fNoPxn",
        "outputId": "db3d1675-66b9-47ee-b852-27cb58615977"
      },
      "source": [
        "dff3.show()"
      ],
      "execution_count": 117,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+---+\n",
            "|  C|A_B|A_D|\n",
            "+---+---+---+\n",
            "|  2|  1|  3|\n",
            "+---+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m9HsVIXBQ2Uf"
      },
      "source": [
        "https://www.sqlservercentral.com/blogs/schema-evolution-solved-using-delta-lake-databricks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "srl-gWoA_JnM"
      },
      "source": [
        "https://kontext.tech/column/spark/381/schema-merging-evolution-with-parquet-in-spark-and-hive"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6WDLsZXVrAKD"
      },
      "source": [
        "def harmonize_schemas(standard_schema, source_file):\r\n",
        "    left_types = {f.name: f.dataType for f in standard_schema}\r\n",
        "    right_types = {f.name: f.dataType for f in source_file.schema}\r\n",
        "    left_fields = set((f.name, f.dataType,f.nullable) for f in standard_schema)\r\n",
        "    right_fields = set((f.name, f.dataType,f.nullable) for f in source_file.schema)\r\n",
        "    for l_name, l_type ,l_nullable in left_fields.difference(right_fields):\r\n",
        "      if (l_name in right_types):\r\n",
        "        r_type=right_types[l_name];\r\n",
        "        if (l_type!=r_type):\r\n",
        "          source_file = source_file.withColumn(l_name, source_file[l_name].cast(l_type));\r\n",
        "        if (l_nullable!=r_nullable):\r\n",
        "          source_file.schema[l_name].nullable = l_nullable;\r\n",
        "      source_file = source_file.withColumn(l_name, lit(None).cast(l_type));\r\n",
        "      source_file.schema[l_name].nullable = l_nullable;\r\n",
        "      print(\"Mismatches with standard_schema :\",(l_name,l_type))\r\n",
        "    source_file=source_file.select(list(left_types.keys()));\r\n",
        "    return source_file;"
      ],
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bit5DEERPdez"
      },
      "source": [
        "standard_schema=dff2.schema"
      ],
      "execution_count": 123,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "unHED9Us-s4K"
      },
      "source": [
        "correct_df2=harmonize_schemas(standard_schema, dff3)"
      ],
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6-uikVicPzXv",
        "outputId": "cc1dfd34-37f3-468c-928e-3bfb9920f558"
      },
      "source": [
        "correct_df2.show()"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "|  C|A_B|\n",
            "+---+---+\n",
            "|  2|  1|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fTCgn8rECe2B",
        "outputId": "c8a15bb1-22a5-4059-a838-89f29996a693"
      },
      "source": [
        "correct_df2.union(dff2).show()"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "+---+---+\n",
            "|  C|A_B|\n",
            "+---+---+\n",
            "|  2|  1|\n",
            "|  2|  1|\n",
            "+---+---+\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}